# CLIPLabeler.label_masks()

[Back to CLIP Labeler](../vision_clip_labeler.md)

## Related User Story
"As a user, I want EDI to understand my image's composition so it knows what can be safely edited." (from PRD)

## Function Signature
`label_masks(image, masks) -> List[Entity]`

## Parameters
- `image` - The original image that was segmented
- `masks` - A list of mask objects generated by the SAM analyzer

## Returns
- `List[Entity]` - A list of Entity objects with labels and confidence scores

## Step-by-step Logic
1. Take each mask region from the input masks list
2. Crop the corresponding region from the original image
3. Use CLIP to encode each cropped region
4. Compare mask regions to text labels via CLIP similarity
5. Assign labels based on CLIP text similarity (comparing to predefined labels like "sky", "building", "person", etc.)
6. Calculate confidence scores for each assigned label
7. Return a list of entities with IDs, labels, confidence scores, and other properties

## Performance Optimizations
- Batch processing of masks for efficiency
- Caching of text embeddings for common labels
- Skipping processing of very small masks (<2% of image area)

## Input/Output Data Structures
### Entity Object
An Entity object contains:
- ID (e.g., "sky_0")
- Label (e.g., "sky")
- Confidence score
- Bounding box coordinates
- Dominant color
- Area percentage

## See Docs

```python
import numpy as np
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass
import torch
import cv2
from PIL import Image as PILImage

@dataclass
class Entity:
    """
    An Entity object contains:
    - ID (e.g., "sky_0")
    - Label (e.g., "sky")
    - Confidence score
    - Bounding box coordinates
    - Dominant color
    - Area percentage
    """
    id: str
    label: str
    confidence: float
    bbox: Tuple[int, int, int, int]  # (x1, y1, x2, y2)
    dominant_color: Tuple[int, int, int]  # RGB values
    area_percentage: float

class CLIPLabeler:
    def __init__(self, text_labels: List[str] = None, min_area_percentage: float = 2.0):
        """
        Initialize the CLIP labeler with predefined text labels.
        In a real implementation, this would load a CLIP model.
        """
        # Default list of common labels, but can be overridden
        self.text_labels = text_labels or [
            "person", "sky", "tree", "building", "car", "road", 
            "grass", "water", "mountain", "animal", "flower", 
            "window", "door", "furniture", "food", "sky", "cloud", 
            "sun", "moon", "star", "bird", "insect", "fish"
        ]
        self.min_area_percentage = min_area_percentage  # Skip masks smaller than 2% of image area
        
        # For this implementation, we're simulating CLIP functionality
        # In a real implementation, you would load an actual CLIP model here
        # self.clip_model, self.preprocess = load_clip_model()

    def label_masks(self, image: np.ndarray, masks: List[np.ndarray]) -> List[Entity]:
        """
        Label masks using CLIP similarity.
        
        This method:
        1. Takes each mask region from the input masks list
        2. Crops the corresponding region from the original image
        3. Use CLIP to encode each cropped region
        4. Compare mask regions to text labels via CLIP similarity
        5. Assign labels based on CLIP text similarity (comparing to predefined labels like "sky", "building", "person", etc.)
        6. Calculate confidence scores for each assigned label
        7. Return a list of entities with IDs, labels, confidence scores, and other properties
        """
        entities = []
        
        # Get image dimensions to calculate area percentages
        if len(image.shape) == 3:
            img_height, img_width = image.shape[:2]
        else:
            img_height, img_width = image.shape
        
        total_image_area = img_height * img_width
        
        for i, mask in enumerate(masks):
            # Skip very small masks to optimize performance
            mask_area = np.sum(mask > 0)
            area_percentage = (mask_area / total_image_area) * 100
            
            if area_percentage < self.min_area_percentage:
                continue
            
            # Extract the region of interest from the original image using the mask
            roi = self._extract_roi(image, mask)
            
            if roi is not None and roi.size > 0:
                # Calculate bounding box of the mask region
                bbox = self._calculate_bbox(mask)
                
                # Calculate dominant color in the masked region
                dominant_color = self._calculate_dominant_color(image, mask)
                
                # Simulate CLIP labeling (in a real implementation, this would use actual CLIP)
                label, confidence = self._simulate_clip_labeling(roi, mask)
                
                # Create entity object
                entity_id = f"{label}_{i}"
                entity = Entity(
                    id=entity_id,
                    label=label,
                    confidence=confidence,
                    bbox=bbox,
                    dominant_color=dominant_color,
                    area_percentage=area_percentage
                )
                
                entities.append(entity)
        
        return entities
    
    def _extract_roi(self, image: np.ndarray, mask: np.ndarray) -> np.ndarray:
        """
        Extract the region of interest from the image using the mask.
        """
        # Ensure mask and image have compatible dimensions
        if len(mask.shape) == 2 and len(image.shape) == 3:
            # Match the mask to the image dimensions
            if mask.shape[:2] != image.shape[:2]:
                # Resize mask to match image if necessary
                mask = cv2.resize(mask, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_NEAREST)
        
        # Apply mask to image
        masked_image = np.zeros_like(image)
        if len(image.shape) == 3:
            # Color image
            for c in range(image.shape[2]):
                masked_image[:, :, c] = np.where(mask > 0, image[:, :, c], 0)
        else:
            # Grayscale image
            masked_image = np.where(mask > 0, image, 0)
        
        # Find bounding box of the masked region
        coords = np.column_stack(np.where(mask > 0))
        if coords.size == 0:
            return None  # No foreground pixels in mask
        
        y1, x1 = coords.min(axis=0)
        y2, x2 = coords.max(axis=0)
        
        # Extract the ROI
        roi = masked_image[y1:y2+1, x1:x2+1]
        return roi

    def _calculate_bbox(self, mask: np.ndarray) -> Tuple[int, int, int, int]:
        """
        Calculate the bounding box of the mask region.
        """
        coords = np.column_stack(np.where(mask > 0))
        if coords.size == 0:
            return (0, 0, 0, 0)
        
        y1, x1 = coords.min(axis=0)
        y2, x2 = coords.max(axis=0)
        return (x1, y1, x2+1, y2+1)

    def _calculate_dominant_color(self, image: np.ndarray, mask: np.ndarray) -> Tuple[int, int, int]:
        """
        Calculate the dominant color in the masked region.
        """
        # Apply mask to image to get only masked pixels
        if len(image.shape) == 3:
            # Color image
            masked_pixels = image[mask > 0]
            if len(masked_pixels) > 0:
                # Calculate average color
                avg_color = np.mean(masked_pixels, axis=0)
                return tuple(map(int, avg_color))
            else:
                # Default color if no valid pixels
                return (128, 128, 128)
        else:
            # Grayscale image - return a gray color
            masked_pixels = image[mask > 0]
            if len(masked_pixels) > 0:
                avg_value = int(np.mean(masked_pixels))
                return (avg_value, avg_value, avg_value)
            else:
                return (128, 128, 128)

    def _simulate_clip_labeling(self, roi: np.ndarray, mask: np.ndarray) -> Tuple[str, float]:
        """
        Simulate CLIP labeling. In a real implementation, this would use actual CLIP model.
        """
        # For the simulation, we'll use simple heuristics based on visual features
        # In a real implementation, this would:
        # 1. Preprocess the ROI for CLIP
        # 2. Encode the image with CLIP vision encoder
        # 3. Encode the text labels with CLIP text encoder
        # 4. Calculate similarity scores
        # 5. Return the label with highest similarity and the confidence
        
        # Simple heuristic based on shape, color, and position to simulate CLIP labeling
        mask_area = np.sum(mask > 0)
        height, width = mask.shape
        
        # Calculate aspect ratio
        if width > 0 and height > 0:
            aspect_ratio = width / height
        else:
            aspect_ratio = 1.0
        
        # Look at location in image to help with classification
        coords = np.column_stack(np.where(mask > 0))
        if coords.size > 0:
            y_coords = coords[:, 0]
            avg_y = np.mean(y_coords)
            position_ratio = avg_y / height if height > 0 else 0.5
        else:
            position_ratio = 0.5
        
        # Simple heuristic classification based on geometric properties
        label = "object"
        confidence = 0.5
        
        # If it's in the upper part of the image, might be sky
        if position_ratio < 0.3 and aspect_ratio > 1.5:
            label = "sky"
            confidence = 0.8
        
        # If it's a tall, narrow object, might be a person or tree
        elif aspect_ratio < 0.7:
            if mask_area > 0.05 * height * width:  # Significant size
                label = "person" if 0.6 < aspect_ratio < 0.9 else "tree"
                confidence = 0.7
            else:
                label = "tree"  # Assume tree if narrow
                confidence = 0.6
        
        # If it's a large horizontal area, might be ground/grass
        elif aspect_ratio > 2.0 and position_ratio > 0.6:
            label = "grass"
            confidence = 0.75
        
        # Add some randomness to make it more realistic
        import random
        confidence = min(0.95, max(0.1, confidence + random.uniform(-0.1, 0.1)))
        
        # Select label from our predefined list (in a real implementation, it would match to any text label)
        if label not in self.text_labels:
            # Find the closest label from our predefined list based on some similarity
            # For now, we'll just pick a random one if not found
            import random
            label = random.choice(self.text_labels)
        
        return label, confidence

# Example usage:
if __name__ == "__main__":
    # Create a mock image (in practice this would be a real image)
    mock_image = np.random.randint(0, 255, (300, 300, 3), dtype=np.uint8)
    
    # Create mock masks (in practice these would come from SAM)
    mock_masks = [
        np.zeros((300, 300)),  # Background initially
        np.zeros((300, 300)),  # Another region
    ]
    
    # Add some "mask" regions to the arrays
    mock_masks[0][50:100, 50:100] = 1  # A small square region
    mock_masks[1][150:250, 100:200] = 1  # A larger rectangular region
    
    # Create CLIP labeler and label the masks
    labeler = CLIPLabeler()
    entities = labeler.label_masks(mock_image, mock_masks)
    
    print("Labeled Entities:")
    for i, entity in enumerate(entities):
        print(f"Entity {i}:")
        print(f"  ID: {entity.id}")
        print(f"  Label: {entity.label}")
        print(f"  Confidence: {entity.confidence:.2f}")
        print(f"  BBox: {entity.bbox}")
        print(f"  Dominant Color: {entity.dominant_color}")
        print(f"  Area Percentage: {entity.area_percentage:.2f}%")
        print()
```